---
layout: post
title: hadoop-map-reduce
categories: sddtc tech
---

> **How Many Maps?**  

* The number of maps is usually driven by the total size of the inputs, that is, the total number of blocks of the input files.  
* The right level of parallelism for maps seems to be around 10-100 maps per-node, although it has been set up to 300 maps for very cpu-light map tasks. Task setup takes a while, so it is best if the maps take at least a minute to execute.  
* Thus, if you expect 10TB of input data and have a blocksize of 128MB, you'll end up with 82,000 maps, unless Configuration.set(MRJobConfig.NUM_MAPS, int) (which only provides a hint to the framework) is used to set it even higher.  

一次mr需要多少个map取决于文件大小和文件块大小

> **How to get the input file name in the mapper in a Hadoop program?**

```java
String fileName = ((FileSplit) context.getInputSplit()).getPath().getName();
```

> **hive建表**

* hive建表时默认的分隔符是'\001',若在建表的时候没有指明分隔符，load文件的时候文件的分隔符需要是'\001'的，若文件分隔符不是'\001'，程序不会报错，但表查询的结果会全部为'NULL'.
* 用vi编辑器Ctrl+v然后Ctrl+a就可以通过键盘输入'\001'.

hive修改表字段类型(例如：column_name int -> string)  

```sql
alter table xxx change column_name column_name string
```
